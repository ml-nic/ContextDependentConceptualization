{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\Anaconda3\\envs\\kbqa\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from Conceptualizer import Conceptualizer\n",
    "from LDA import LDA\n",
    "\n",
    "\n",
    "def evaluate(word1, word2, sentence1, sentence2):\n",
    "    concept_distribution1 = conceptualizer.conceptualize(sentence1, word1, eval=True)\n",
    "    concept_distribution2 = conceptualizer.conceptualize(sentence2, word2, eval=True)\n",
    "    if concept_distribution1 is None or concept_distribution2 is None:\n",
    "        return None\n",
    "\n",
    "    vector1 = np.zeros(np.max([len(concept_distribution1), len(concept_distribution2)]))\n",
    "    vector2 = np.zeros(np.max([len(concept_distribution1), len(concept_distribution2)]))\n",
    "    if len(concept_distribution1) == np.max([len(concept_distribution1), len(concept_distribution2)]):\n",
    "        counter = 0\n",
    "        for concept1 in concept_distribution1:\n",
    "            vector1[counter] = concept1[1]\n",
    "            for concept2 in concept_distribution2:\n",
    "                if concept1[0] == concept2[0]:\n",
    "                    vector2[counter] = concept2[1]\n",
    "                    break\n",
    "            counter += 1\n",
    "    elif len(concept_distribution2) == np.max([len(concept_distribution1), len(concept_distribution2)]):\n",
    "        counter = 0\n",
    "        for concept2 in concept_distribution2:\n",
    "            vector2[counter] = concept2[1]\n",
    "            for concept1 in concept_distribution1:\n",
    "                if concept2[0] == concept1[0]:\n",
    "                    vector1[counter] = concept1[1]\n",
    "                    break\n",
    "            counter += 1\n",
    "    else:\n",
    "        print(\"jl\")\n",
    "        print(vector1)\n",
    "        print(vector2)\n",
    "        print(concept_distribution1)\n",
    "        print(concept_distribution2)\n",
    "        print(word1, sentence1)\n",
    "        print(word2, sentence2)\n",
    "        exit()\n",
    "    try:\n",
    "        estimated_similarity = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "    except ValueError as e:\n",
    "        print(vector1)\n",
    "        print(vector2)\n",
    "        print(concept_distribution1)\n",
    "        print(concept_distribution2)\n",
    "        print(word1, sentence1)\n",
    "        print(word2, sentence2)\n",
    "        print(vector1.reshape(1, -1))\n",
    "        print(vector2.reshape(1, -1))\n",
    "        raise e\n",
    "\n",
    "    return estimated_similarity\n",
    "\n",
    "\n",
    "def get_eval_set():\n",
    "    eval_set = []\n",
    "    with(open('../ratings.txt', 'r')) as file:\n",
    "        for line in file.readlines():\n",
    "            splitted_line = line.split('\\t')\n",
    "            id = splitted_line[0]\n",
    "            word1 = splitted_line[1]\n",
    "            pos_word1 = splitted_line[2]\n",
    "            word2 = splitted_line[3]\n",
    "            pos_word2 = splitted_line[4]\n",
    "            word1_in_context = splitted_line[5]\n",
    "            word2_in_context = splitted_line[6]\n",
    "            average_human_rating = splitted_line[7]\n",
    "            ten_individual_human_ratings = [rating.replace('\\n', '') for rating in splitted_line[7:len(splitted_line)]]\n",
    "            word1_in_context = word1_in_context.replace('<b> ', '').replace(' </b>', '')\n",
    "            word2_in_context = word2_in_context.replace('<b> ', '').replace(' </b>', '')\n",
    "            eval_set.append((id, word1, word2, word1_in_context, word2_in_context, average_human_rating))\n",
    "    return eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n",
      "WARNING:root:failed to load state from F:/Not_Uploaded/conceptualization_eval/models/ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200.gensim.state: [Errno 2] No such file or directory: 'F:/Not_Uploaded/conceptualization_eval/models/ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200.gensim.state'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200 :\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LdaMallet' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-776baa628431>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mestimated_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mestimated_similarity\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mnone_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d0efac294938>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(word1, word2, sentence1, sentence2)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mconcept_distribution1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconceptualizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconceptualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mconcept_distribution2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconceptualizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconceptualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconcept_distribution1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mconcept_distribution2\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Nicolas\\IdeaProjects\\ContextDependentConceptualization\\src\\Conceptualizer.py\u001b[0m in \u001b[0;36mconceptualize\u001b[1;34m(self, sentence, instance, eval)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m#                                                                  sentence.encode('utf-8'), e))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mprobabilities_of_concepts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__calculate_probs_of_concepts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcepts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;31m#print(probabilities_of_concepts)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprobabilities_of_concepts\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities_of_concepts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Nicolas\\IdeaProjects\\ContextDependentConceptualization\\src\\Conceptualizer.py\u001b[0m in \u001b[0;36m__calculate_probs_of_concepts\u001b[1;34m(self, concepts, sentence)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m#    summm += prob_of_topic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m#print(summm)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mtopic_terms_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mtopics_terms_proba_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_terms_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mprobs_of_topics_for_given_concept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopics_terms_proba_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconcept\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LdaMallet' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "MODEL_BASE_DIR = 'F:/Not_Uploaded/conceptualization_eval/models/'\n",
    "MODEL_FILE_EXTENSION = '.gensim'\n",
    "model_names = [\n",
    "    # 'ldamodel_50',\n",
    "    'ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200'\n",
    "]\n",
    "model_stat = []\n",
    "eval_set = get_eval_set()\n",
    "for model_name in model_names:\n",
    "    print(\"\\nTest\", model_name, ':')\n",
    "    lda = LDA()\n",
    "    lda.load(MODEL_BASE_DIR + model_name + MODEL_FILE_EXTENSION)\n",
    "    conceptualizer = Conceptualizer(lda)\n",
    "\n",
    "    none_counter = 0\n",
    "    estimated_similarities = []\n",
    "    real_similarities = []\n",
    "\n",
    "    for element in eval_set:\n",
    "        estimated_similarity = evaluate(element[1], element[2], element[3], element[4])\n",
    "        if estimated_similarity is None:\n",
    "            none_counter += 1\n",
    "            continue\n",
    "        estimated_similarities.append(float(estimated_similarity[0][0]))\n",
    "        real_similarities.append(float(element[5]))\n",
    "        if (int(element[0]) - 1) % 100 == 0 and int(element[0]) > 10:\n",
    "            print('now at entry with id', element[0], 'tooks', (time.time() - start_time), 'seconds')\n",
    "            estimated_similarities_np = np.array(estimated_similarities)\n",
    "            real_similarities_np = np.array(real_similarities)\n",
    "            pearson_score = pearsonr(estimated_similarities_np, real_similarities_np)\n",
    "            print(pearson_score)\n",
    "            np.save('real_similarites' + model_name, real_similarities_np)\n",
    "            np.save('estimated_similarities' + model_name, estimated_similarities_np)\n",
    "    print(\"finish\")\n",
    "    estimated_similarities_np = np.array(estimated_similarities)\n",
    "    real_similarities_np = np.array(real_similarities)\n",
    "    pearson_score = pearsonr(estimated_similarities_np, real_similarities_np)\n",
    "    print(\"pearson_score\", pearson_score)\n",
    "\n",
    "    np.save('real_similarites' + model_name, real_similarities_np)\n",
    "    np.save('estimated_similarities' + model_name, estimated_similarities_np)\n",
    "    model_stat.append((model_name, pearson_score, none_counter))\n",
    "    print('finished took', (time.time() - start_time), 'seconds for this model')\n",
    "print(model_stat)\n",
    "print('finished all models, took', (time.time() - start_time), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n",
      "WARNING:root:failed to load state from F:/Not_Uploaded/conceptualization_eval/models/ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200.gensim.state: [Errno 2] No such file or directory: 'F:/Not_Uploaded/conceptualization_eval/models/ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200.gensim.state'\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "MODEL_BASE_DIR = 'F:/Not_Uploaded/conceptualization_eval/models/'\n",
    "MODEL_FILE_EXTENSION = '.gensim'\n",
    "lda.load(MODEL_BASE_DIR +'ldamodel_simple_mallet_20_10_keep_300000_gensimstop_topics200'+ MODEL_FILE_EXTENSION)\n",
    "\n",
    "#gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda.ldamodel, gamma_threshold=0.001, iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_as_tuple = {'id': 'some_id', 'fb_user_id': 'fb_user', 'personwvch_id': 'personwvch', 'comparecounter': 12, 'datetime_of_matching': 'datetime'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_t = [row_as_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'comparecounter': 12,\n",
       "  'datetime_of_matching': 'datetime',\n",
       "  'fb_user_id': 'fb_user',\n",
       "  'id': 'some_id',\n",
       "  'personwvch_id': 'personwvch'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'datetime_of_matching': 'datetime', 'comparecounter': 12, 'fb_user_id': 'fb_user', 'personwvch_id': 'personwvch', 'id': 'some_id'}\n"
     ]
    }
   ],
   "source": [
    "for idx, item in ((idx,item) for idx, item in enumerate(list_t) if item[\"personwvch_id\"] == \"personwvch\"):\n",
    "    print(idx, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'datetime_of_matching': 'datetime', 'comparecounter': 12, 'fb_user_id': 'fb_user', 'personwvch_id': 'personwvch', 'id': 'some_id'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comparecounter': 12,\n",
       " 'datetime_of_matching': 'datetime',\n",
       " 'fb_user_id': 'fb_user',\n",
       " 'id': 'some_id',\n",
       " 'personwvch_id': 'personwvch'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:kbqa]",
   "language": "python",
   "name": "conda-env-kbqa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
