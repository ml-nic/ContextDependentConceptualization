{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\Anaconda3\\envs\\kbqa\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import urllib\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "probase_cache = {}\n",
    "translate_cache = {}\n",
    "\n",
    "\n",
    "def split_tweet_in_words(tweet):\n",
    "    real_words = []\n",
    "\n",
    "    words = re.findall(r'\\'|’|\"|”|“|»|«|\\(|\\)|\\[|\\]|\\{|\\}:;|[^\\'’\"”“»«\\(\\)\\[\\]\\{\\}\\s:;]+', tweet)\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if word.startswith(\"...\"):\n",
    "            real_words.append(word[:3])\n",
    "            append_if_not_empty(real_words, word[3:])\n",
    "        if word.startswith((\"\\\"\", \"(\", \"[\", \"{\", \"<\", \"«\", \"…\", \"“\")):\n",
    "            real_words.append(word[:1])\n",
    "            word = word[1:]\n",
    "        if word.endswith(\"...\"):\n",
    "            append_if_not_empty(real_words, word[:-3])\n",
    "            real_words.append(word[-3:])\n",
    "        elif word.endswith((\".\", \",\", \":\", \";\", \"]\" \")\", \"}\", \"!\", \"?\", \"\\\"\", \">\", \"»\", \"…\", \"”\")):\n",
    "            append_if_not_empty(real_words, word[:-1])\n",
    "            real_words.append(word[-1:])\n",
    "        else:\n",
    "            append_if_not_empty(real_words, word)\n",
    "    return real_words\n",
    "\n",
    "\n",
    "def append_if_not_empty(list, item):\n",
    "    if item:\n",
    "        list.append(item)\n",
    "\n",
    "\n",
    "def get_entity_probase_concepts(entity):\n",
    "    if entity in probase_cache:\n",
    "        return probase_cache[entity]\n",
    "    url = 'https://concept.research.microsoft.com/api/Concept/ScoreByProb?instance={}&topK=20&api_key=eT5luCbmII34ZvpPVs7HxtbUU1cFcE12'\n",
    "    request_url = url.format(urllib.parse.quote_plus(entity))\n",
    "    response = requests.get(request_url, verify=False)\n",
    "    concepts = response.json()\n",
    "    if len(concepts) == 0:\n",
    "        request_url = url.format(urllib.parse.quote_plus(entity))\n",
    "        response = requests.get(request_url, verify=False)\n",
    "        concepts = response.json()\n",
    "    # for now let's keep it simple\n",
    "    probase_cache[entity] = concepts\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LDA():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = get_stop_words('en')\n",
    "\n",
    "    def load(self, file):\n",
    "        self.ldamodel = gensim.models.ldamodel.LdaModel.load(file)\n",
    "\n",
    "    def update(self, document_folder, save_to):\n",
    "        corpus, dictionary = self.create_lda_corpus(document_folder)\n",
    "        # update LDA model\n",
    "        self.ldamodel.update(corpus)\n",
    "        print('Finished updating')\n",
    "        self.ldamodel.save(save_to)\n",
    "\n",
    "    def train(self, nr_of_topics, document_folder, save_to, training_iterations = 20):\n",
    "        corpus, dictionary = self.create_lda_corpus(document_folder)\n",
    "\n",
    "        # generate LDA model\n",
    "        self.ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=nr_of_topics, id2word=dictionary, passes=training_iterations)\n",
    "        print('Finished training')\n",
    "        self.ldamodel.save(save_to)\n",
    "\n",
    "    def create_lda_corpus(self, document_folder):\n",
    "        # load documents\n",
    "        files = os.listdir(document_folder)\n",
    "        doc_set = []\n",
    "        for file in files:\n",
    "            with open(document_folder + '/' + file, \"r\", encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    l = line.strip()\n",
    "                    if len(l) > 0:\n",
    "                        doc_set.append(l)\n",
    "\n",
    "        print(\"Read {} documents\".format(len(doc_set)))\n",
    "\n",
    "        # list for tokenized documents in loop\n",
    "        texts = []\n",
    "\n",
    "        # loop through document list\n",
    "        for i in doc_set:\n",
    "            # add tokens to list\n",
    "            texts.append(self.process_document_content(i))\n",
    "\n",
    "        # turn our tokenized documents into a id <-> term dictionary\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        return ([dictionary.doc2bow(text) for text in texts], dictionary)\n",
    "\n",
    "    def process_document_content(self, doc):\n",
    "        raw = doc.lower()\n",
    "        tokens = self.tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in self.stop_words]\n",
    "\n",
    "        return stopped_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Conceptualizer():\n",
    "    def __init__(self, lda):\n",
    "        self.lda = lda\n",
    "        self.ldamodel = lda.ldamodel\n",
    "\n",
    "    def conceptualize(self, sentence, instance):\n",
    "        probase_concepts = get_entity_probase_concepts(instance)\n",
    "        if len(probase_concepts) == 0:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # check context\n",
    "            words = split_tweet_in_words(sentence.lower())\n",
    "            instance_words = split_tweet_in_words(instance.lower())\n",
    "            i = words.index(instance_words[0])\n",
    "            larger_probase_concepts = get_entity_probase_concepts(\" \".join(words[max(i - 1, 0):i + len(instance_words)]))\n",
    "            if len(larger_probase_concepts) > 0:\n",
    "                return None\n",
    "            larger_probase_concepts = get_entity_probase_concepts(\" \".join(words[i:i + len(instance_words) + 1]))\n",
    "            if len(larger_probase_concepts) > 0:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print('Error getting larger concepts for {} in {}: {}'.format(instance.encode('utf-8'), sentence.encode('utf-8'), e))\n",
    "\n",
    "        probabilities = []\n",
    "        alpha = 1000\n",
    "        for name in collections.OrderedDict(sorted(probase_concepts.items())):\n",
    "            if name in self.ldamodel.id2word.token2id.keys():\n",
    "                probability = probase_concepts[name] / alpha\n",
    "                concept_topics = self.ldamodel.get_term_topics(name, minimum_probability=0)\n",
    "                concept_probabilities = [0] * self.ldamodel.num_topics\n",
    "                for nr, prob in concept_topics:\n",
    "                    concept_probabilities[nr] = prob\n",
    "                bow = self.ldamodel.id2word.doc2bow(self.lda.process_document_content(sentence))\n",
    "                doc_topics = self.ldamodel.get_document_topics(bow, minimum_probability=0)\n",
    "                summ = 0\n",
    "                for k in doc_topics:\n",
    "                    summ += concept_probabilities[k[0]] * k[1]\n",
    "                probabilities.append((name, summ * probability))\n",
    "\n",
    "        print(probabilities)\n",
    "        if len(probabilities) == 0:\n",
    "            return None\n",
    "        found_concept = max(probabilities, key=lambda item: item[1])[0]\n",
    "        return found_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('candidate', 7.2788384779448036e-08), ('celebrity', 2.9674991200152272e-10), ('democrat', 3.26039879304097e-08), ('leader', 2.5843048461607351e-07), ('person', 1.1772555810108534e-07), ('personality', 1.7598007182319703e-09), ('politician', 2.0864288599554689e-07), ('president', 3.2535116587479946e-07)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'president'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "lda = LDA(RegexpTokenizer(r'\\w+'))\n",
    "#lda.load('../models/ldamodel_50.gensim')\n",
    "lda.load('../models/ldamodel_topics100_trainiter20_train_en.gensim')\n",
    "conceptualizer = Conceptualizer(lda)\n",
    "conceptualizer.conceptualize('When was Barack Obama born?', 'Barack Obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {'elitist': 0.052054794520547946, 'person': 0.1095890410958904, 'prominent black leader': 0.0410958904109589, 'notable liberal': 0.03287671232876712, 'personality': 0.06027397260273973, 'great leader': 0.030136986301369864, 'celebrity': 0.0273972602739726, 'famous person': 0.030136986301369864, 'political leader': 0.049315068493150684, 'satanic asslickers': 0.0273972602739726, 'candidate': 0.0547945205479452, 'democrat': 0.0547945205479452, 'presidential candidate': 0.024657534246575342, 'college student': 0.024657534246575342, 'name': 0.038356164383561646, 'u s politician': 0.0410958904109589, 'world leader': 0.052054794520547946, 'leader': 0.1095890410958904, 'politician': 0.11506849315068493, 'president': 0.024657534246575342}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate 0.0547945205479452\n",
      "celebrity 0.0273972602739726\n",
      "college student 0.024657534246575342\n",
      "democrat 0.0547945205479452\n",
      "elitist 0.052054794520547946\n",
      "famous person 0.030136986301369864\n",
      "great leader 0.030136986301369864\n",
      "leader 0.1095890410958904\n",
      "name 0.038356164383561646\n",
      "notable liberal 0.03287671232876712\n",
      "person 0.1095890410958904\n",
      "personality 0.06027397260273973\n",
      "political leader 0.049315068493150684\n",
      "politician 0.11506849315068493\n",
      "president 0.024657534246575342\n",
      "presidential candidate 0.024657534246575342\n",
      "prominent black leader 0.0410958904109589\n",
      "satanic asslickers 0.0273972602739726\n",
      "u s politician 0.0410958904109589\n",
      "world leader 0.052054794520547946\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "for name in collections.OrderedDict(sorted(a.items())):\n",
    "    print(name, a[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
