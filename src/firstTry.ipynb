{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "\n",
    "import requests\n",
    "\n",
    "cache = {}\n",
    "\n",
    "\n",
    "def get_concepts_of_instance_by_probase(instance, use_cache=True):\n",
    "    \"\"\"\n",
    "    Fetches the concept and the probabilities for a given instance by probase.\n",
    "    :param instance: the instance, for which the concepts should be requested\n",
    "    :param use_cache: if true a cache for instances and corresponding concepts is used, to avoid unnecessary requests\n",
    "    :return: the concepts and their probability\n",
    "    \"\"\"\n",
    "    if use_cache == True and instance in cache:\n",
    "        return cache[instance]\n",
    "\n",
    "    probase_url = 'https://concept.research.microsoft.com/api/Concept/ScoreByProb?instance={}&topK=20&api_key=eT5luCbmII34ZvpPVs7HxtbUU1cFcE12'\n",
    "    requestUrl = probase_url.format(urllib.parse.quote_plus(instance))\n",
    "    response = requests.get(requestUrl)\n",
    "    concepts = response.json()\n",
    "    return concepts\n",
    "\n",
    "\n",
    "def appendIfNotEmpty(list, item):\n",
    "    \"\"\"\n",
    "    Append item to list, if item is not None. in place\n",
    "    :param list: the list, where the item should been appended to\n",
    "    :param item: the item which should been appended to the list\n",
    "    \"\"\"\n",
    "    if item:\n",
    "        list.append(item)\n",
    "\n",
    "\n",
    "def split_text_in_words(text):\n",
    "    \"\"\"\n",
    "    Splits a given text into words\n",
    "    :param text: the text which should be splited into words\n",
    "    :return: a list containing the splitted words\n",
    "    \"\"\"\n",
    "    real_words = []\n",
    "\n",
    "    words = re.findall(r'\\'|’|\"|”|“|»|«|\\(|\\)|\\[|\\]|\\{|\\}:;|[^\\'’\"”“»«\\(\\)\\[\\]\\{\\}\\s:;]+', text)\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if word.startswith(\"...\"):\n",
    "            real_words.append(word[:3])\n",
    "            appendIfNotEmpty(real_words, word[3:])\n",
    "        if word.startswith((\"\\\"\", \"(\", \"[\", \"{\", \"<\", \"«\", \"…\", \"“\")):\n",
    "            real_words.append(word[:1])\n",
    "            word = word[1:]\n",
    "        if word.endswith(\"...\"):\n",
    "            appendIfNotEmpty(real_words, word[:-3])\n",
    "            real_words.append(word[-3:])\n",
    "        elif word.endswith((\".\", \",\", \":\", \";\", \"]\" \")\", \"}\", \"!\", \"?\", \"\\\"\", \">\", \"»\", \"…\", \"”\")):\n",
    "            appendIfNotEmpty(real_words, word[:-1])\n",
    "            real_words.append(word[-1:])\n",
    "        else:\n",
    "            appendIfNotEmpty(real_words, word)\n",
    "    return real_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nicolas\\Anaconda3\\envs\\kbqa\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "def iter_wiki(wiki_dump_file, min_length_of_article=50, ignore_namespaces=None):\n",
    "    \"\"\"\n",
    "    Iterator over wiki_dump_file.\n",
    "    Returns title and tokens for next article in dump file.\n",
    "    Ignores short articles.\n",
    "    Ignores meta articles, throug given namespaces.\n",
    "    Default namespaces are 'Wikipedia', 'Category', 'File', 'Portal', 'Template', 'MediaWiki', 'User', 'Help', 'Book', 'Draft'\n",
    "    :param wiki_dump_file: the dump file\n",
    "    :param min_length_of_article: the min number of words in the next article. Default = 50\n",
    "    :param ignore_namespaces: list of namespaces which should be ignored.\n",
    "    :return: title, tokens\n",
    "    \"\"\"\n",
    "    if ignore_namespaces is None:\n",
    "        ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for title, text, pageid in _extract_pages(smart_open(wiki_dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < min_length_of_article or any(title.startswith(namespace + ':') for namespace in ignore_namespaces):\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield title, tokens\n",
    "\n",
    "\n",
    "# TODO compare simple_preprocess to my own preprocess\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Preprocess and then tokenize a given text\n",
    "    :param text: the text which should be tokenized.\n",
    "    :return: the token of the given text, after preprocess the text\n",
    "    \"\"\"\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self):\n",
    "        self.stop_words = get_stop_words('en')\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"\n",
    "        Loads a LDA model from a given file\n",
    "        :param model_file: the file which contains the model, which should be loaded\n",
    "        \"\"\"\n",
    "        self.ldamodel = gensim.models.ldamulticore.LdaMulticore.load(model_file)\n",
    "\n",
    "    def train_on_document_folder(self, num_topics, document_folder, model_outputfile, training_iterations=20):\n",
    "        \"\"\"\n",
    "        Trains a new lda model, based on a folder with different document.\n",
    "        Each document in a different file.\n",
    "        :param num_topics: the number of topics, which should be generated\n",
    "        :param document_folder: the folder, which contains the documents\n",
    "        :param model_outputfile: the file in which the trained model should be saved\n",
    "        :param training_iterations: the number of LDA training iterations\n",
    "        \"\"\"\n",
    "        corpus, dictionary = self.__create_lda_corpus_based_on_document_folder(document_folder)\n",
    "\n",
    "        self.ldamodel = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                                                                passes=training_iterations)\n",
    "        self.ldamodel.save(model_outputfile)\n",
    "\n",
    "    def generate_bow_of_wiki_dump(self, wiki_dump_file, bow_output_file, dict_output_file):\n",
    "        doc_stream = (tokens for _, tokens in iter_wiki(wiki_dump_file))\n",
    "        id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "        print(id2word_wiki)\n",
    "        id2word_wiki.filter_extremes(no_below=20, no_above=0.1, keep_n=None)\n",
    "        print(id2word_wiki)\n",
    "        wiki_corpus = WikiCorpus(wiki_dump_file, id2word_wiki)\n",
    "        print(\"save bow...\")\n",
    "        gensim.corpora.MmCorpus.serialize(bow_output_file, wiki_corpus)\n",
    "        print(\"save dict\")\n",
    "        id2word_wiki.save(dict_output_file)\n",
    "\n",
    "    def train_on_wiki_dump(self, num_topics, bow_path, dict_path, model_outputfile, training_iterations=20,\n",
    "                           max_docs=None):\n",
    "        \"\"\"\n",
    "        Trains a new LDA model based on a wikipedia dump or any other dump in the same format.\n",
    "        The dump could be zipped.\n",
    "        :param num_topics: the number of topics, which should be generated\n",
    "        :param bow_path: the path inclusive filename, where the bag of words should be saved\n",
    "        :param dict_path: the path incl. filename, where the dictionary should be saved\n",
    "        :param model_outputfile: the file in which the trained model should be stored\n",
    "        :param training_iterations: the number of LDA training iterations\n",
    "        :param max_docs: the number of how many docs should be used for training, if None all docs are used\n",
    "        \"\"\"\n",
    "        print(\"load bow...\")\n",
    "        mm_corpus = gensim.corpora.MmCorpus(bow_path)\n",
    "        print(\"load dict...\")\n",
    "        id2word_wiki = gensim.corpora.Dictionary.load(dict_path)\n",
    "        clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, max_docs)\n",
    "        print(\"start trainig\")\n",
    "        self.ldamodel = gensim.models.ldamulticore.LdaMulticore(clipped_corpus, num_topics=num_topics,\n",
    "                                                                id2word=id2word_wiki, passes=training_iterations)\n",
    "        print(\"save model\")\n",
    "        self.ldamodel.save(model_outputfile)\n",
    "\n",
    "    def __create_lda_corpus_based_on_document_folder(self, document_folder):\n",
    "        \"\"\"\n",
    "        Creates a corpus and the corresponding dictionary, for a given document folder\n",
    "        :param document_folder: the folder, which contains the documents\n",
    "        :return: the corpus and the dictionary\n",
    "        \"\"\"\n",
    "        files = os.listdir(document_folder)\n",
    "        doc_set = []\n",
    "        for file in files:\n",
    "            with open(document_folder + '/' + file, \"r\", encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    l = line.strip()\n",
    "                    if len(l) > 0:\n",
    "                        doc_set.append(l)\n",
    "\n",
    "        print(\"Finished reading {} documents\".format(len(doc_set)))\n",
    "\n",
    "        # list for tokenized documents in loop\n",
    "        texts = self.preprocess_documents(doc_set)\n",
    "\n",
    "        # turn our tokenized documents into a id <-> term dictionary\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        return ([dictionary.doc2bow(text) for text in texts], dictionary)\n",
    "\n",
    "    def preprocess_documents_original(self, docs):\n",
    "        \"\"\"\n",
    "        Tokenize and remove stop words of given documents\n",
    "        :param docs: collection which contains the documents\n",
    "        :return: the preprocessed texts\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for doc in docs:\n",
    "            raw = doc.lower()\n",
    "            tokens = tokenize(raw)\n",
    "\n",
    "            stopped_tokens = [i for i in tokens if not i in self.stop_words]\n",
    "            texts.append(stopped_tokens)\n",
    "        return texts\n",
    "\n",
    "    def preprocess_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Preprocess given documents.\n",
    "\n",
    "        removes meta-articles,\n",
    "        ignores short documents, to avoid unwanted word 2 word connections,\n",
    "        removes stop words\n",
    "        :param documents: collection of to be processed documents\n",
    "        \"\"\"\n",
    "        namespaces = ['Wikipedia', 'Category', 'File', 'Portal', 'Template', 'MediaWiki', 'User', 'Help', 'Book',\n",
    "                      'Draft']\n",
    "        namespaces = [namespace.lower() for namespace in namespaces]\n",
    "\n",
    "        # remove_short_documents\n",
    "        # remove_articles_where_title_specific_namespaces\n",
    "        texts = []\n",
    "        for document in documents:\n",
    "            if len(document) >= 200:\n",
    "                if document.title not in namespaces:\n",
    "                    tokens = tokenize(document.lower())\n",
    "                    # Stoplist cleaning:\n",
    "                    cleaned_tokens = [token for token in tokens if token not in self.stop_words]\n",
    "                    texts.append(tokens)\n",
    "        # Remove most frequent and less frequent words\n",
    "        # for word in all documents:\n",
    "        #    if word appears in more than 10 % of the articles:\n",
    "        #        remove(word) from whole corpora\n",
    "        #    if word apperas in less than 20 articles:\n",
    "        #        remove(word) from corpora\n",
    "\n",
    "        # Additional possible steps\n",
    "        # filter by length\n",
    "        # lemmatization\n",
    "        # stemming\n",
    "        # parts of speech\n",
    "\n",
    "        # then keep top n words # recommended 50.000 - 100.000\n",
    "        return texts\n",
    "\n",
    "    def update_on_document_folder(self, document_folder, model_output_file):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        Updates the current LDA model, trained on the given documents\n",
    "        :param document_folder: the folder, which contains the new documents\n",
    "        :param model_output_file: the outputfile, where the updated model should be stored\n",
    "        \"\"\"\n",
    "        corpus, dictionary = self.__create_lda_corpus_based_on_document_folder(document_folder)\n",
    "        print('start updating')\n",
    "        self.ldamodel.update(corpus)\n",
    "        print('finished updating. Now save model')\n",
    "        self.ldamodel.save(model_output_file)\n",
    "\n",
    "\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator over wiki corpus\n",
    "        :return: bag-of-words format = list of `(token_id, token_count)` 2-tuples\n",
    "        \"\"\"\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "#lda.load('../models/ldamodel_topics100_trainiter20_train_en.gensim')\n",
    "lda.load('../models/ldamodel_50.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utilities import get_concepts_of_instance_by_probase\n",
    "from utilities import split_text_in_words\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "class Conceptualizer():\n",
    "    def __init__(self, lda):\n",
    "        self.lda = lda\n",
    "        self.ldamodel = lda.ldamodel\n",
    "\n",
    "    def conceptualize(self, sentence, instance):\n",
    "        \"\"\"\n",
    "        Conceptualize the given instance in the given context (sentence)\n",
    "        :param sentence: a sentence as context\n",
    "        :param instance: the instance, which should be conceptualized in the given context\n",
    "        :return: the most likely concept for the intance in the given context\n",
    "        \"\"\"\n",
    "        concepts = get_concepts_of_instance_by_probase(instance)\n",
    "        if len(concepts) == 0:  # TODO\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # check context\n",
    "            words = split_text_in_words(sentence.lower())\n",
    "            instance_words = split_text_in_words(instance.lower())\n",
    "            i = words.index(instance_words[0])\n",
    "            largerProbaseConcepts = get_concepts_of_instance_by_probase(\n",
    "                \" \".join(words[max(i - 1, 0):i + len(instance_words)]))\n",
    "            if len(largerProbaseConcepts) > 0:\n",
    "                pass\n",
    "                #return None\n",
    "            largerProbaseConcepts = get_concepts_of_instance_by_probase(\" \".join(words[i:i + len(instance_words) + 1]))\n",
    "            if len(largerProbaseConcepts) > 0:\n",
    "                pass\n",
    "                #return None\n",
    "        except Exception as e:\n",
    "            print('Error getting larger concepts for {} in {}: {}'.format(instance.encode('utf-8'),\n",
    "                                                                          sentence.encode('utf-8'), e))\n",
    "        probabilities_of_concepts = self.__calculate_probs_of_concepts(concepts, sentence)\n",
    "        \n",
    "        most_likely_concept = max(probabilities_of_concepts, key=lambda item: item[1])[0]\n",
    "        return most_likely_concept\n",
    "    \n",
    "    def __calculate_probs_of_concepts(self, concepts, sentence):\n",
    "        \"\"\"\n",
    "        Calculates for each concept the probability of the concept for the given sentence\n",
    "        :param concepts: the concepts and their probability\n",
    "        :param sentence: the given sentence\n",
    "        :return: the concepts and ther probabilities\n",
    "        \"\"\"\n",
    "        probabilities_of_concepts = []\n",
    "        for concept in collections.OrderedDict(sorted(concepts.items())):#concepts:\n",
    "            if concept not in self.ldamodel.id2word.token2id.keys():\n",
    "                continue\n",
    "            prob_c_given_w = concepts[concept]\n",
    "            \n",
    "            #topics_of_concept = self.ldamodel.get_term_topics(concept, minimum_probability=0.0) # phi\n",
    "            #probs_of_topics_for_given_concept = [0] * self.ldamodel.num_topics\n",
    "            #summm = 0\n",
    "            #for topic_id, prob_of_topic in topics_of_concept:\n",
    "            #    probs_of_topics_for_given_concept[topic_id] = prob_of_topic\n",
    "            #    summm += prob_of_topic\n",
    "            #print(summm)\n",
    "            topic_terms_ = self.ldamodel.state.get_lambda()\n",
    "            topics_terms_proba_ = np.apply_along_axis(lambda x: x/x.sum(), 1, topic_terms_)\n",
    "            probs_of_topics_for_given_concept = topics_terms_proba_[:,self.ldamodel.id2word.token2id[concept]]\n",
    "            \n",
    "            #for topic_id in range(100):\n",
    "            #    print(np.sum(topics_terms_proba_[topic_id,:]))\n",
    "            \n",
    "            bag_of_words = self.ldamodel.id2word.doc2bow(simple_preprocess(sentence))\n",
    "            # topic_distribution_for_given_bow\n",
    "            topics_of_text = self.ldamodel.get_document_topics(bag_of_words, minimum_probability=0)\n",
    "            sum = 0\n",
    "            for topic_id, prob_of_topic in topics_of_text:\n",
    "                sum += probs_of_topics_for_given_concept[topic_id] * prob_of_topic\n",
    "            prob_c_given_w_z = prob_c_given_w * sum\n",
    "            \n",
    "            probabilities_of_concepts.append((concept, prob_c_given_w_z))\n",
    "        return probabilities_of_concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leader'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "conceptualizer = Conceptualizer(lda)\n",
    "conceptualizer.conceptualize(\"When was Barack Obama born?\", \"Barack Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30202"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.ldamodel.id2word.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'person' in lda.ldamodel.id2word.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
